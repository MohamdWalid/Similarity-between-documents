# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsyKiuDosDUrkmXam2_-ZJqKBg4CB2rZ
"""

import pandas as pd
import nltk
nltk.download('stopwords')

documentA = """Computer science is the study of computers and what they can do—the
inherent powers and limitations of abstract computers, the design and characteristics of real computers, and the innumerable applications of computers to solving problems. Computer scientists seek to understand how to represent
and to reason about processes and information. They create languages for
representing these phenomena and develop methods for analyzing and
creating the phenomena. They create abstractions, including abstractions
that are themselves used to compose, manipulate, and represent other
abstractions. They study the symbolic representation, implementation,
manipulation, and communication of information. They create, study,
experiment on, and improve real-world computational and information
systems—the working hardware and software artifacts that embody the
computing capabilities. They develop models, methods, and technologies
to help design, realize, and operate these artifacts. They amplify human
intellect through the automation of rote tasks and construction of new
capabilities. Computer hardware and software have been central to computer science since its origins. However, computer science also encompasses the
study and more general application of principles and theories rooted in or
motivated by hardware and software phenomena. Computer science has
thus come to encompass topics once distinctly part of other disciplines,
such as mathematics, originally motivated by computing and conceptual
questions of information-handling tasks such as natural-language processing. Computer science research is often intimately intertwined with
application, as the need to solve practical problems drives new theoretical
breakthroughs. Computer science’s striking research advances have touched our lives
in profound ways as we work, learn, play, and communicate. Even though
computer science is a relatively young field, born only within the past
seven decades, the pace of innovation in it has been extraordinary. Once
esoteric, expensive, and reserved for specialized tasks, computers are now
seemingly everywhere. Applications and technologies that are now fixtures in many people’s lives and work (such as office automation,
e-commerce, and search engines) were nonexistent or barely visible just a
decade ago. The personal computer itself was first introduced less then
three decades ago, yet most office workers are now assigned a PC as a
matter of course, and roughly half of all households in the United States
own at least one. Computers are central to the daily operations of banks,
brokerage houses, airlines, telephone systems, and supermarkets. Even
more computers lurk hidden inside handheld cell phones, personal digital
assistants, and automobiles; for example, a typical midmarket automobile
contains a network and dozens of processors. As the size and cost of
computer hardware shrink, computers will continue to proliferate even
more widely (see Hill in Chapter 2). All these computers have provided
society with tremendous social and economic benefits even as they pose
new challenges for public policy and social organizations. Advances in computer science have also led to fundamental changes
in many scientific and engineering disciplines, enabling, for example,
complex numerical calculations that would simply be infeasible if
attempted by hand. convenient bytes (sequences of 8 bits, representing characters such as
letters) to which the proper interpretation is applied. A digital image of
any one of Van Gogh’s paintings of sunflowers (call it Sunflowers, for
short) divides the continuous painted canvas into many small rectangular
regions called pixels and gives the (approximate) color at each of these.
The collection of pixels can, in turn, be thought of as a sequence of bits. Bit
sequences, being a “digital” or discrete-valued representation, cannot
fully represent precise, real-valued or “analog” information (e.g., the precise amount of yellow in a sunflower). Practically, though, this apparent
limitation can be overcome by using a sufficiently large enough number
of bits, to, for example, represent the analog information as precisely as
the eye can see or the ear can hear. This sequence of bits can be processed in one way so that a person can
see the Sunflowers image on a display or processed another way for a
printer to print an image. This sequence of bits could be sent in an e-mail
message to a friend or posted to a Web page. In principle, the same
sequence of bits could be passed to an audio interpreter; however, the
audio produced by Sunflowers would not sound very good, since this bit
sequence is unlikely to represent anything reasonable in the symbol system used by the audio player. Sunflowers could be executed as a program
on the computer, since programs are themselves bit strings; again, however, the result will probably not produce anything meaningful, since it is
unlikely the painting’s representation is a sequence that will do something useful. Computer science often involves formulating and manipulating
abstractions, which are coordinated sets of definitions that capture different aspects of a particular entity—from broad concept to its detailed
representation in bits—and the relations through which some of the definitions refine others or provide additional concepts to help realize others.
One aspect of abstraction is that sequences of bits take on specific
useful interpretations that make sense only in particular contexts. For
example, a bit sequence can be thought of—in some contexts—as an
integer in binary notation; using that abstraction, it makes sense to divide
the integer by 3 and get another sequence of bits that is the quotient. But
in a context that abstracts bit sequences as images, it would not be reasonable to divide Sunflowers by 3 and get something meaningful, nor would it
make sense to divide a DNA sequence by 3. But the abstraction of bits as,
for example, images, should permit operations that make no sense on
integers, such as cropping (removing bits that represent pixels at the edges
of an image) or applying a watercolor filter in an image-manipulation
program. Similarly, the abstraction of bits as DNA sequences should permit analysis of differences among plant species that would make no sense
for integers or images. 
 """



documentB = """To view all technical guidance documents regarding COVID-19, please go to this webpage.
WHO has developed interim guidance for laboratory diagnosis, advice on the use of masks during home care and
in health care settings in the context of COVID-19 outbreak, clinical management, infection prevention and
control in health care settings, home care for patients with suspected novel coronavirus, risk communication and
community engagement and Global Surveillance for human infection with COVID-19.
WHO is working closely with International Air Transport Association (IATA) and have jointly developed a
guidance document to provide advice to cabin crew and airport workers, based on country queries. The
guidance can be found on the IATA webpage.
WHO has been in regular and direct contact with Member States where cases have been reported. WHO is also
informing other countries about the situation and providing support as requested.
WHO is working with its networks of researchers and other experts to coordinate global work on surveillance,
epidemiology, mathematical modelling, diagnostics and virology, clinical care and treatment, infection
prevention and control, and risk communication. WHO has issued interim guidance for countries, which are
updated regularly. A general introduction to emerging respiratory viruses, including novel coronaviruses (available in
Arabic, Chinese, English, French, Russian, Spanish, Hindi, Indian Sign Language, Persian, Portuguese,
Serbian and Turkish);
Clinical care for Severe Acute Respiratory Infections (available in English, French, Russian, Indonesian
and Vietnamese);
Health and safety briefing for respiratory diseases - ePROTECT (available in Chinese, English, French,
Russian, Spanish, Indonesian and Portuguese);
Infection Prevention and Control for Novel Coronavirus (COVID-19) (available in Chinese, English, French,
Russian, Spanish, Indonesian, Italian, Japanese, Portuguese and Serbian); and
COVID-19 Operational Planning Guidelines and COVID-19 Partners Platform to support country
preparedness and response (available in English and coming soon in additional languages).
WHO is providing guidance on early investigations, which are critical in an outbreak of a new virus. The data
collected from the protocols can be used to refine recommendations for surveillance and case definitions, to
characterize the key epidemiological transmission features of COVID-19, help understand spread, severity,
spectrum of disease, impact on the community and to inform operational models for implementation of
countermeasures such as case isolation, contact tracing and isolation.
"""

documentC = """Artificial intelligence (AI) is the intelligence of machines and the branch of computer science that
aims to create it. It is the science and engineering of making intelligent machines, especially
intelligent computer programs. It is related to the task of using computers to understand human
intelligence, but AI does not confine itself to methods that are biologically observable.
 While there are many different definitions, AI textbooks define the field as "the study and design
of intelligent agents" where an intelligent agent is a system that perceives its environment and
takes actions that maximize its chances of success. John McCarthy, who coined the term in
1956, defines it as "the science and engineering of making intelligent machines."
The field was founded on the claim that a central property of humans, intelligence—
the sapience of Homo sapiens—can be so precisely described that it can be simulated by a
machine. This raises philosophical issues about the nature of the mind and the ethics of
creating artificial beings, issues which have been addressed by myth, fiction and
philosophy since antiquity. Artificial intelligence has been the subject of optimism, but has also
suffered setbacks and, today, has become an essential part of the technology industry,
providing the heavy lifting for many of the most difficult problems in computer science. 
Artificial intelligence is not always about simulating human intelligence. Most work in AI involves
studying the problems the world presents to intelligence rather than studying people or animals.
AI researchers are free to use methods that are not observed in people or that involve much
more computing than people can do.
Another important fact about artificial intelligence is that computer programs have no IQ
(Intelligence Quotient). This is because IQ is based on the rates at which intelligence develops
in children. It is the ratio of the age at which a child normally makes a certain score to the child's
age. The scale is extended to adults in a suitable way. IQ correlates well with various measures
of success or failure in life, but making computers that can score high on IQ tests would be
weakly correlated with their usefulness. For example, the ability of a child to repeat back a long
sequence of digits correlates well with other intellectual abilities, perhaps because it measures
how much information the child can compute with at once. However, ``digit span'' is trivial for
even extremely limited computers. Evidence of Artificial Intelligence folklore can be traced back to ancient Egypt, but with the
development of the electronic computer in 1941, the technology finally became available to
create machine intelligence. The term artificial intelligence was first coined in 1956, at the
Dartmouth conference, and since then Artificial Intelligence has expanded because of the
theories and principles developed by its dedicated researchers. Although the computer provided the technology necessary for AI, it was not until the early
1950's that the link between human intelligence and machines was really observed. The first
observations were made on the principle of feedback theory. The most familiar example of
feedback theory is the thermostat. It controls the temperature of an environment by gathering
the actual temperature of the house, comparing it to the desired temperature, and responding
by turning the heat up or down. What was so important about this research into feedback loops
was that it theorized that all intelligent behavior was the result of feedback mechanisms. This
discovery influenced much of the early development of AI.
In late 1955 The Logic Theorist, was developed considered by many to be the first AI program.
The program, representing each problem as a tree model, would attempt to solve it by selecting
the branch that would most likely result in the correct conclusion. The impact that it made on
both the public and the field of AI has made it a crucial stepping stone in developing the AI field.
In 1956 John McCarthy regarded as the father of AI, organized a conference to draw the talent
and expertise of others interested in machine intelligence for a month of brainstorming. He
invited them to Vermont for "The Dartmouth summer research project on artificial intelligence."
From that point on, because of McCarthy, the field would be known as Artificial intelligence.
Although not a huge success, the Dartmouth conference did bring together the founders in AI,
and served to lay the groundwork for the future of AI research. 


"""

numbers = ['0','1','2','3','4','5','6','7','8','9']
punc = ['+','-','.','*','/','"\"','=','-','_',')','(','&','^','%','$','#','@','!','~','[',']','{','}','|','"',':',';',',','<','>','.','/','?','\n']

for i in range(0,len(documentA)):
  if(documentA[i] in numbers or documentA[i] in punc):
    documentA = documentA.replace(documentA[i],' ')

for i in range(0,len(documentB)):
  if(documentB[i] in numbers or documentB[i] in punc):
    documentB = documentB.replace(documentB[i],' ')
    

for i in range(0,len(documentC)):
  if(documentC[i] in numbers or documentC[i] in punc):
    documentC = documentC.replace(documentC[i],' ')

print(documentB)

"""Split to Words"""

bowA1 = documentA.split(' ')
bowB1 = documentB.split(' ')
bowC1 = documentC.split(' ')

bowA,bowB,bowC = [] , [] , []
for i in range(0,len(bowA1)):
     if bowA1[i] != '':
       bowA.append(bowA1[i])
for i in range(0,len(bowB1)):
     if bowB1[i] != '':
       bowB.append(bowB1[i])
for i in range(0,len(bowC1)):
     if bowC1[i] != '':
       bowC.append(bowC1[i])

"""Delet Stop_Words """

stopwords = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
  txt_clean = text.lower() if text.lower() not in stopwords else ''
  return txt_clean

bowA1 = list(map(lambda x : remove_stopwords(x),bowA))
bowB1 = list(map(lambda x : remove_stopwords(x),bowB))
bowC1 = list(map(lambda x : remove_stopwords(x),bowC))
bowA,bowB,bowC = [] , [] , []
for i in range (0,len(bowA1)):
     if bowA1[i] !='':
       bowA.append(bowA1[i])

for i in range (0,len(bowB1)):
     if bowB1[i] !='':
       bowB.append(bowB1[i])

for i in range (0,len(bowC1)):
     if bowC1[i] !='':
       bowC.append(bowC1[i])

print(bowA)
print(bowB)
print(bowC)

uniqueWords = set(bowA).union(set(bowB))
uniqueWords2 = uniqueWords.intersection(set(bowC))
print(uniqueWords)
print(uniqueWords2)

numofwordsA = dict.fromkeys(uniqueWords,0)
numofwordsB = dict.fromkeys(uniqueWords,0)
numofwordsC = dict.fromkeys(uniqueWords,0)

print(numofwordsA)
print(numofwordsB)
print(numofwordsC)

for word1 in bowA:
  numofwordsA[word1] +=1

for word2 in bowB:
  numofwordsB[word2] +=1

for word3 in bowC:
  if(word3 in uniqueWords):
       numofwordsC[word3] +=1
  

print(numofwordsA)
print(numofwordsB)
print(numofwordsC)

def TF(wordDict,bow):
  tfDict = {}
  bow_count = len(bow)
  for word , count in wordDict.items():
    tfDict[word] = count / float(bow_count)
  return tfDict

tfA = TF(numofwordsA,bowA)
tfB = TF(numofwordsB,bowB)
tfC = TF(numofwordsC,bowC)
print(tfA)
print(tfB)
print(tfC)

def computeIDF(documents):
   import math
   N = len(documents)
   idfDict = dict.fromkeys(documents[0].keys(), 0)
   for document in documents:
      for word, val in document.items():
         if val > 0:
            idfDict[word] += 1
   for word, val in idfDict.items():
        idfDict[word] = math.log(N / float(val+1))+1

   return idfDict

idfs = computeIDF([numofwordsA, numofwordsB])
idfs

def computeTFIDF(tfBagOfWords, idfs):
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    return tfidf

tfidfA = computeTFIDF(tfA, idfs)
print(tfidfA)

tfidfB = computeTFIDF(tfB, idfs)
print(tfidfB)

tfidfC = computeTFIDF(tfC, idfs)
print(tfidfC)

df = pd.DataFrame([tfidfA,tfidfB,tfidfC])
df

import numpy as np
document1 = df.iloc[0:1,:]
document2 = df.iloc[1:2,:]
document3 = df.iloc[2:3,:]
dis1 = np.power(np.sum(np.power(np.array(document1) - np.array(document3),2)),0.5)
dis2 = np.power(np.sum(np.power(np.array(document2) - np.array(document3),2)),0.5)

print(f"Distance between CS and AI = {dis1}")
print(f"Distance between Covid and AI = {dis2}")